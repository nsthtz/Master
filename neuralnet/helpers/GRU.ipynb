{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import dataset as dataset\n",
    "import datapreparation as datp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import copy\n",
    "import importlib\n",
    "import GRU as gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#reload imports\n",
    "importlib.reload(datp)\n",
    "importlib.reload(gru)\n",
    "neuraldir = os.path.abspath(os.path.join(os.curdir, os.pardir))\n",
    "data = dataset.pianoroll_dataset_batch(neuraldir+'\\\\datasets\\\\training\\\\voicesonly\\\\piano_roll_fs_1')\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize Generalist\n",
    "## Generalist(input_size, hidden_size, num_tags)\n",
    "model_generalist = gru.Generalist(128, 256, int(data.num_tags()))\n",
    "model_generalist.optimizer = torch.optim.Adam(model_generalist.parameters())\n",
    "model_generalist.loss_log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCE\n",
      "Epoch: 0\n",
      "Epoch loss: 381.5345\n",
      "Epoch: 1\n",
      "Epoch loss: 374.4489\n",
      "Epoch: 2\n",
      "Epoch loss: 374.7692\n",
      "Epoch: 3\n",
      "Epoch loss: 344.4734\n",
      "Epoch: 4\n",
      "Epoch loss: 312.0000\n",
      "Epoch: 5\n",
      "Epoch loss: 289.0405\n",
      "Epoch: 6\n",
      "Epoch loss: 283.8056\n",
      "Epoch: 7\n",
      "Epoch loss: 279.6585\n",
      "Epoch: 8\n",
      "Epoch loss: 275.4201\n",
      "Epoch: 9\n",
      "Epoch loss: 271.9426\n",
      "Epoch: 10\n",
      "Epoch loss: 268.1259\n",
      "Epoch: 11\n",
      "Epoch loss: 265.1706\n",
      "Epoch: 12\n",
      "Epoch loss: 262.3533\n",
      "Epoch: 13\n",
      "Epoch loss: 259.8831\n",
      "Epoch: 14\n",
      "Epoch loss: 257.6252\n",
      "Epoch: 15\n",
      "Epoch loss: 255.8107\n",
      "Epoch: 16\n",
      "Epoch loss: 253.4803\n",
      "Epoch: 17\n",
      "Epoch loss: 251.4592\n",
      "Epoch: 18\n",
      "Epoch loss: 249.7511\n",
      "Epoch: 19\n",
      "Epoch loss: 247.8715\n",
      "Epoch: 20\n",
      "Epoch loss: 246.7833\n",
      "Epoch: 21\n",
      "Epoch loss: 245.0430\n",
      "Epoch: 22\n",
      "Epoch loss: 243.6870\n",
      "Epoch: 23\n",
      "Epoch loss: 242.3598\n",
      "Epoch: 24\n",
      "Epoch loss: 240.9431\n",
      "Epoch: 25\n",
      "Epoch loss: 239.5610\n",
      "Epoch: 26\n",
      "Epoch loss: 238.0099\n",
      "Epoch: 27\n",
      "Epoch loss: 237.5122\n",
      "Epoch: 28\n",
      "Epoch loss: 235.7836\n",
      "Epoch: 29\n",
      "Epoch loss: 235.2853\n",
      "Epoch: 30\n",
      "Epoch loss: 234.8903\n",
      "Epoch: 31\n",
      "Epoch loss: 233.9055\n",
      "Epoch: 32\n",
      "Epoch loss: 232.5766\n",
      "Epoch: 33\n",
      "Epoch loss: 232.0135\n",
      "Epoch: 34\n",
      "Epoch loss: 230.8521\n",
      "Epoch: 35\n",
      "Epoch loss: 230.3652\n",
      "Epoch: 36\n",
      "Epoch loss: 230.0027\n",
      "Epoch: 37\n",
      "Epoch loss: 229.1737\n",
      "Epoch: 38\n",
      "Epoch loss: 228.2189\n",
      "Epoch: 39\n",
      "Epoch loss: 227.2908\n",
      "Epoch: 40\n",
      "Epoch loss: 227.1890\n",
      "Epoch: 41\n",
      "Epoch loss: 226.2145\n",
      "Epoch: 42\n",
      "Epoch loss: 225.5770\n",
      "Epoch: 43\n",
      "Epoch loss: 224.1292\n",
      "Epoch: 44\n",
      "Epoch loss: 223.7794\n",
      "Epoch: 45\n",
      "Epoch loss: 223.5578\n",
      "Epoch: 46\n",
      "Epoch loss: 223.1817\n",
      "Epoch: 47\n",
      "Epoch loss: 223.3780\n",
      "Epoch: 48\n",
      "Epoch loss: 221.9416\n",
      "Epoch: 49\n",
      "Epoch loss: 221.3458\n",
      "Epoch: 50\n",
      "Epoch loss: 221.2618\n",
      "Epoch: 51\n",
      "Epoch loss: 220.5060\n",
      "Epoch: 52\n",
      "Epoch loss: 220.0307\n",
      "Epoch: 53\n",
      "Epoch loss: 219.5697\n",
      "Epoch: 54\n",
      "Epoch loss: 218.8229\n",
      "Epoch: 55\n",
      "Epoch loss: 218.6842\n",
      "Epoch: 56\n",
      "Epoch loss: 218.7079\n",
      "Epoch: 57\n",
      "Epoch loss: 218.8695\n",
      "Epoch: 58\n",
      "Epoch loss: 216.9641\n",
      "Epoch: 59\n",
      "Epoch loss: 217.2646\n",
      "Epoch: 60\n",
      "Epoch loss: 217.5907\n",
      "Epoch: 61\n",
      "Epoch loss: 216.4410\n",
      "Epoch: 62\n",
      "Epoch loss: 216.4481\n",
      "Epoch: 63\n",
      "Epoch loss: 215.4521\n",
      "Epoch: 64\n",
      "Epoch loss: 215.2820\n",
      "Epoch: 65\n",
      "Epoch loss: 215.5476\n",
      "Epoch: 66\n",
      "Epoch loss: 215.2949\n",
      "Epoch: 67\n",
      "Epoch loss: 216.3132\n",
      "Epoch: 68\n",
      "Epoch loss: 215.4767\n",
      "Epoch: 69\n",
      "Epoch loss: 214.6858\n",
      "Epoch: 70\n",
      "Epoch loss: 215.0727\n",
      "Epoch: 71\n",
      "Epoch loss: 214.1544\n",
      "Epoch: 72\n",
      "Epoch loss: 214.4099\n",
      "Epoch: 73\n",
      "Epoch loss: 214.1241\n",
      "Epoch: 74\n",
      "Epoch loss: 212.6438\n",
      "Epoch: 75\n",
      "Epoch loss: 213.3593\n",
      "Epoch: 76\n",
      "Epoch loss: 212.9594\n",
      "Epoch: 77\n",
      "Epoch loss: 212.7160\n",
      "Epoch: 78\n",
      "Epoch loss: 212.8978\n",
      "Epoch: 79\n",
      "Epoch loss: 212.7355\n",
      "Epoch: 80\n",
      "Epoch loss: 213.5775\n",
      "Epoch: 81\n",
      "Epoch loss: 211.8808\n",
      "Epoch: 82\n",
      "Epoch loss: 213.7611\n",
      "Epoch: 83\n",
      "Epoch loss: 212.4368\n",
      "Epoch: 84\n",
      "Epoch loss: 212.1337\n",
      "Epoch: 85\n",
      "Epoch loss: 211.7301\n",
      "Epoch: 86\n",
      "Epoch loss: 212.1879\n",
      "Epoch: 87\n",
      "Epoch loss: 212.5059\n",
      "Epoch: 88\n",
      "Epoch loss: 212.3685\n",
      "Epoch: 89\n",
      "Epoch loss: 211.5059\n",
      "Epoch: 90\n",
      "Epoch loss: 211.5176\n",
      "Epoch: 91\n",
      "Epoch loss: 211.7979\n",
      "Epoch: 92\n",
      "Epoch loss: 211.5231\n",
      "Epoch: 93\n",
      "Epoch loss: 210.7398\n",
      "Epoch: 94\n",
      "Epoch loss: 211.8445\n",
      "Epoch: 95\n",
      "Epoch loss: 211.7710\n",
      "Epoch: 96\n",
      "Epoch loss: 211.3366\n",
      "Epoch: 97\n",
      "Epoch loss: 211.3597\n",
      "Epoch: 98\n",
      "Epoch loss: 210.9690\n",
      "Epoch: 99\n",
      "Epoch loss: 211.2464\n",
      "Epoch: 100\n",
      "Epoch loss: 210.9215\n",
      "Epoch: 101\n",
      "Epoch loss: 209.9831\n",
      "Epoch: 102\n",
      "Epoch loss: 210.3779\n",
      "Epoch: 103\n",
      "Epoch loss: 210.9052\n",
      "Epoch: 104\n",
      "Epoch loss: 211.4001\n",
      "Epoch: 105\n",
      "Epoch loss: 210.5255\n",
      "Epoch: 106\n",
      "Epoch loss: 210.0635\n",
      "Epoch: 107\n",
      "Epoch loss: 211.2430\n",
      "Epoch: 108\n",
      "Epoch loss: 210.9548\n",
      "Epoch: 109\n",
      "Epoch loss: 211.0306\n",
      "Epoch: 110\n",
      "Epoch loss: 210.7913\n",
      "Epoch: 111\n",
      "Epoch loss: 210.9291\n",
      "Epoch: 112\n",
      "Epoch loss: 210.1827\n",
      "Epoch: 113\n",
      "Epoch loss: 210.9763\n",
      "Epoch: 114\n",
      "Epoch loss: 210.4744\n",
      "Epoch: 115\n",
      "Epoch loss: 211.0177\n",
      "Epoch: 116\n",
      "Epoch loss: 211.2058\n",
      "Epoch: 117\n",
      "Epoch loss: 211.0106\n",
      "Epoch: 118\n",
      "Epoch loss: 210.3673\n",
      "Epoch: 119\n",
      "Epoch loss: 210.5430\n",
      "Epoch: 120\n",
      "Epoch loss: 210.7971\n",
      "Epoch: 121\n",
      "Epoch loss: 210.4362\n",
      "Epoch: 122\n",
      "Epoch loss: 210.3383\n",
      "Epoch: 123\n",
      "Epoch loss: 211.0187\n",
      "Epoch: 124\n"
     ]
    }
   ],
   "source": [
    "## Train the generalist\n",
    "num_epochs = 1000\n",
    "## Returns state for transferred training\n",
    "state = gru.train_sequence(model_generalist, num_epochs, data, model_generalist.optimizer, model_generalist.loss_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the Generalist state\n",
    "dir_path = os.path.dirname(os.path.realpath(gru.__file__))\n",
    "torch.save(state, dir_path+'\\\\Chorals.pt')\n",
    "\n",
    "## Current best state 1000 epochs, 256 hidden_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of epochs run:  200\n",
      "Epoch number of lowest loss:  131\n",
      "Loss for best epoch 36.107497591525316\n"
     ]
    }
   ],
   "source": [
    "## Print Generalist state\n",
    "print('Total number of epochs run: ', len(model_generalist.loss_log))\n",
    "print('Epoch number of lowest loss: ', model_generalist.loss_log.index(min(model_generalist.loss_log)))\n",
    "print('Loss for best epoch', min(model_generalist.loss_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA\n"
     ]
    }
   ],
   "source": [
    "## Load saved weights into Generalist model\n",
    "# OLD BACKUP: NOT CHORALS\n",
    "#model_generalist.optimizer = gru.load(model_generalist, filename='Generalist.pt')\n",
    "\n",
    "# CHORALS\n",
    "model_generalist, model_generalist.optimizer = gru.load(model_generalist, filename='Chorals.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate Generalist music\n",
    "embed, matrix = gru.gen_music_seconds_smooth(model_generalist, data[0][0] ,composer=0,fs=1,gen_seconds=30,init_seconds=5, device='cpu')\n",
    "embed\n",
    "\n",
    "\n",
    "\n",
    "#matrix *= 100\n",
    "#datp.piano_roll_to_mid_file(matrix,\"output.mid\",1,74) # 16 Organ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export pianoroll as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "roll = matrix.astype(int)\n",
    "numpy.savetxt(\"test.csv\", roll, fmt='%i', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert csv to midi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\DeepLearning\\\\Master\\\\neuralnet\\\\helpers\\\\test2.mid'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roll *= 100\n",
    "datp.piano_roll_to_mid_file(roll,\"test2.mid\",1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-7fd9026e0e86>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#data2 = dataset.pianoroll_dataset_batch(\"C:\\DeepLearning\\Master\\\\IP\\\\fourpart\\\\output\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdata2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpianoroll_dataset_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:\\DeepLearning\\Master\\\\neural-composer-assignement\\datasets\\\\training\\learnchoralmusic\\\\voicesonly\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mdatp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvisualize_piano_roll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\DeepLearning\\Master\\neural-composer-assignement\\helpers\\dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0minput_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0mtag_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtags_ids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtags\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0moutput_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mone_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "#data2 = dataset.pianoroll_dataset_batch(\"C:\\DeepLearning\\Master\\\\IP\\\\fourpart\\\\output\")\n",
    "data2 = dataset.pianoroll_dataset_batch(\"C:\\DeepLearning\\Master\\\\neural-composer-assignement\\datasets\\\\training\\\\voicesonly\")\n",
    "print(data2[0][0].shape[2])\n",
    "\n",
    "datp.visualize_piano_roll(data2.data[0],fs=1)\n",
    "play, matrix = datp.embed_play_v1(data2.data[0],fs=1)\n",
    "play\n",
    "\n",
    "#matrix *= 100\n",
    "#datp.piano_roll_to_mid_file(matrix,\"output.mid\",1,74) # 16 Organ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 9):\n",
    "    data2 = dataset.pianoroll_dataset_batch(\"C:\\DeepLearning\\Master\\\\IP\\\\fourpart\\\\output\")\n",
    "    print(data2[i][0].shape[2])\n",
    "    play, matrix = datp.embed_play_v1(data2.data[i],fs=1)\n",
    "    \n",
    "    \n",
    "    matrix *= 100\n",
    "    datp.piano_roll_to_mid_file(matrix,\"test\"+str(i)+\".mid\",1,74)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
